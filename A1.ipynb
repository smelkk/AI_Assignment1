{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1_t4V9bnt-hB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ebeeac7105c37bee8e9881688cd16b1",
     "grade": false,
     "grade_id": "cell-e911fa75d4ae6ea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 1: Sci-Kit Learn machine learning preprocessing pipeline\n",
    "\n",
    "This notebook contains a set of exercises that will guide you through the different steps of this assignment. Solutions must be code-based, _i.e._ hard-coded or manually computed results will not be accepted. Remember to write your solutions to each exercise in the dedicated cells and not modify or remove the test cells. When completing all the exercises submit this same notebook back to Moodle in **.ipynb** format.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>About the datasets used in this assignment</b>\n",
    "\n",
    "<u>Context</u>\n",
    "\n",
    "Access to credit is a fundamental aspect of modern financial life, yet the decision to grant a loan is not always transparent. Loan approvals have traditionally been influenced by a variety of applicant characteristics, ranging from income and employment stability to demographic information. The dataset we will work with provides an overview of loan applicants, including personal, financial, and application-related attributes. These features allow us to explore how individual circumstances shape creditworthiness assessments.\n",
    "\n",
    "<u>Content</u>\n",
    "    \n",
    "The columns included in the datasets are: person_age, person_name, person_gender, person_education, employment_type, person_income, person_emp_exp, person_home_ownership, bank_name, account_type, loan_amnt, loan_intent, loan_int_rate, loan_percent_income, cb_person_cred_hist_length, credit_score, previous_loan_defaults_on_file, loan_status\n",
    "\n",
    "Column names are self-explanatory. \n",
    "    \n",
    " <u>Inspiration</u>\n",
    "\n",
    "What are the characteristics of loan applicants that most influence approval decisions? Do factors such as income, employment history, credit score, or loan intent significantly impact whether a loan is approved? Let’s shed light on this critical financial question.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\"><b>Submission deadline:</b> Friday, October 24th, 23:55</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.508510Z",
     "start_time": "2024-09-25T16:13:27.037656Z"
    },
    "deletable": false,
    "editable": false,
    "id": "1gbj1gyT16vl",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c7ba8ebeff4fdcaf2908815838f9823",
     "grade": false,
     "grade_id": "cell-0fcbc57512e78927",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY NOR ADD CODE TO THIS CELL\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jnin/information-systems/refs/heads/main/data/AI1_2025_assignments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a0d38d8bc672001bcbb0092941d91e5",
     "grade": false,
     "grade_id": "cell-960ed6121c802915",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "In the last part of this assignment, we will cover the importance of using three distinct datasets: training, validation, and test. However, for the autograded part of this assignment, we will conduct all calculations using a single dataset, even though this approach is fundamentally flawed, but will make the intial part of the assignment easier to complete. For this reason, there is no accuracy evaluation in the guided part of this assignment.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Exercise 1: Creating the Feature Matrix and Target Array</b>\n",
    "\n",
    "Write the code to create the feature matrix ```X``` and the target array ```y``` from the dataframe ```df```. When creating ```X```, make sure to drop or ignore the irrelevant columns: ```['person_name', 'bank_name', 'credit_score']```.\n",
    "\n",
    "The target variable for this problem is ```loan_status```.\n",
    "\n",
    "<br><i>[0.5 points]</i>\n",
    "</div>\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Python is case-sensitive, so ensure your code matches the required capitalization.\n",
    "Do **not** download the dataset manually. Instead, run the previous cell to load the data directly from the provided link.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.515069Z",
     "start_time": "2024-09-25T16:13:27.512068Z"
    },
    "deletable": false,
    "id": "rnxOTovpEqpu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3416f03882927817c5373de161fb4a59",
     "grade": false,
     "grade_id": "ex1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X = df.drop(columns = [\"person_name\", \"bank_name\", \"credit_score\", \"loan_status\"])\n",
    "y = df[\"loan_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.577276Z",
     "start_time": "2024-09-25T16:13:27.575072Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68ecdd3b3d805baa5acee240c7c43b34",
     "grade": true,
     "grade_id": "test1_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.676729Z",
     "start_time": "2024-09-25T16:13:27.620311Z"
    },
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52885fcba97b2dc1b6f18131fde36661",
     "grade": true,
     "grade_id": "test1_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86da2e763cc281e40839150d05cf4fce",
     "grade": false,
     "grade_id": "cell-a6f3b25eae9b50da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 2: Imputing Missing Values </b> \n",
    "\n",
    "The first step in our preprocessing routine is to handle missing values in the feature matrix ```X```. Write the code to instantiate a ```SimpleImputer``` with a ```most_frequent``` strategy, naming it ```imputer```. Then, test the imputer transforming ```X```, and store the transformed data in a new DataFrame called ```X_imputed```.\n",
    "\n",
    "<br><i>[0.5 points]</i>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.695074Z",
     "start_time": "2024-09-25T16:13:27.690303Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e312a6da82f8c4bc37df98d9182e9662",
     "grade": false,
     "grade_id": "ex2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_gender</th>\n",
       "      <th>person_education</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_exp</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>account_type</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>previous_loan_defaults_on_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Master</td>\n",
       "      <td>contract</td>\n",
       "      <td>71948.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>saving</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>16.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>female</td>\n",
       "      <td>High School</td>\n",
       "      <td>contract</td>\n",
       "      <td>12282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>checking</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>female</td>\n",
       "      <td>High School</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>12438.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>checking</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>12.87</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>79753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>saving</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>15.23</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>male</td>\n",
       "      <td>Master</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>66135.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>saving</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>14.27</td>\n",
       "      <td>0.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  person_age person_gender person_education employment_type person_income  \\\n",
       "0       22.0        female           Master        contract       71948.0   \n",
       "1       21.0        female      High School        contract       12282.0   \n",
       "2       25.0        female      High School   self-employed       12438.0   \n",
       "3       23.0        female         Bachelor   self-employed       79753.0   \n",
       "4       24.0          male           Master      unemployed       66135.0   \n",
       "\n",
       "  person_emp_exp person_home_ownership account_type loan_amnt loan_intent  \\\n",
       "0            0.0                  RENT       saving   35000.0    PERSONAL   \n",
       "1            0.0                   OWN     checking    1000.0   EDUCATION   \n",
       "2            3.0              MORTGAGE     checking    5500.0     MEDICAL   \n",
       "3            0.0                  RENT       saving   35000.0     MEDICAL   \n",
       "4            1.0                  RENT       saving   35000.0     MEDICAL   \n",
       "\n",
       "  loan_int_rate loan_percent_income cb_person_cred_hist_length  \\\n",
       "0         16.02                0.49                        3.0   \n",
       "1         11.14                0.08                        2.0   \n",
       "2         12.87                0.44                        3.0   \n",
       "3         15.23                0.44                        2.0   \n",
       "4         14.27                0.53                        4.0   \n",
       "\n",
       "  previous_loan_defaults_on_file  \n",
       "0                             No  \n",
       "1                            Yes  \n",
       "2                             No  \n",
       "3                             No  \n",
       "4                             No  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X))\n",
    "X_imputed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.735707Z",
     "start_time": "2024-09-25T16:13:27.733506Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dce4ce98b9aa0e83a8639da4e094c460",
     "grade": true,
     "grade_id": "test2_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.781155Z",
     "start_time": "2024-09-25T16:13:27.778626Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1475602122f5703ca985c52402000b8d",
     "grade": true,
     "grade_id": "test2_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a7d2fb2a9ddf060c50f25c5f3c5ab4e",
     "grade": false,
     "grade_id": "cell-c7c37aaa2f33b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 3: Encoding Categorical Features</b> \n",
    "\n",
    "Now that our dataset is free of missing values, let's handle the categorical columns. Create a `OneHotEncoder` object named `one_hot_encoder`. Next, create a DataFrame called `X_categorical` containing the following columns from `X_imputed`: `['person_gender','person_education', 'employment_type','person_emp_exp', 'person_home_ownership', 'loan_intent', 'account_type']`. \n",
    "\n",
    "Test the encoder by transforming the features of `X_categorical`, and store the transformed data in a new DataFrame named `X_categorical_encoded`.\n",
    "\n",
    "<br><i>[0.75 points]</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.833244Z",
     "start_time": "2024-09-25T16:13:27.825751Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b315830f2239f6d81c0964227610ff",
     "grade": false,
     "grade_id": "ex3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_categorical = pd.DataFrame(X_imputed[['person_gender', 'person_education', 'employment_type', 'person_emp_exp', 'person_home_ownership', 'loan_intent', 'account_type']])\n",
    "X_categorical_encoded = one_hot_encoder.fit_transform(X_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.876850Z",
     "start_time": "2024-09-25T16:13:27.874663Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a75ee900d85728951e8b462c9756e4a3",
     "grade": true,
     "grade_id": "test3_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.922580Z",
     "start_time": "2024-09-25T16:13:27.919729Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d97eb6081c645f993eefea334d79e585",
     "grade": true,
     "grade_id": "test3_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:27.967738Z",
     "start_time": "2024-09-25T16:13:27.965682Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f0dfd2e054e861b70d36080185aa039",
     "grade": true,
     "grade_id": "test3_3",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6f2919ddbefc9aa02e6f15905d32040",
     "grade": false,
     "grade_id": "cell-ccef3f102806a3a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 4: Encoding Ordinal Features </b> \n",
    "\n",
    "Next, repeat the process for the ordinal feature `['person_education', 'previous_loan_defaults_on_file']`. First, create a new DataFrame called `X_ordinal` containing this column. Then, instantiate an `OrdinalEncoder` and name it `ordinal_encoder`. \n",
    "\n",
    "Test the encoder by transforming the `X_ordinal` DataFrame, and store the transformed data in a new DataFrame called `X_ordinal_encoded`.\n",
    "\n",
    "<br><i>[0.75 points]</i>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Consider that the integer values assigned to each label should align with a meaningful interpretation of the label's significance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.016989Z",
     "start_time": "2024-09-25T16:13:28.013066Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00ef2091bcd0c6ccc4ed66e901f1fff9",
     "grade": false,
     "grade_id": "ex4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "categories = [['High School', 'Associate', 'Bachelor', 'Master', 'Doctorate'], ['No', 'Yes']]\n",
    "ordinal_encoder = OrdinalEncoder(categories = categories)\n",
    "X_ordinal = X_imputed[['person_education', 'previous_loan_defaults_on_file']]\n",
    "X_ordinal_encoded = pd.DataFrame(ordinal_encoder.fit_transform(X_ordinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.061709Z",
     "start_time": "2024-09-25T16:13:28.059398Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b17bd4ec4b5f3542195269b854037fab",
     "grade": true,
     "grade_id": "test4_1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.105577Z",
     "start_time": "2024-09-25T16:13:28.103588Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06fbab0612fd581dc99cc8b78ae50153",
     "grade": true,
     "grade_id": "test4_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5214ccf8fb8d3d2cb0660a6150dec164",
     "grade": false,
     "grade_id": "cell-9f62321dd9372e29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 5: Combining Feature Transformations </b> \n",
    "\n",
    "Now that we have confirmed the transformations for categorical and ordinal columns, let's use a `ColumnTransformer` to apply them in parallel. Instantiate a `ColumnTransformer` named `transformer`, including both the `OneHotEncoder` and `OrdinalEncoder`. Be sure to specify the correct column names for each transformer.\n",
    "\n",
    "Test your `transformer` by applying it to the `X_imputed` DataFrame, and store the transformed data in a new DataFrame called `X_encoded`.\n",
    "\n",
    "<br><i>[1 points]</i>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.156684Z",
     "start_time": "2024-09-25T16:13:28.148366Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb47640b2984de2eac9bddf8c8c891c6",
     "grade": false,
     "grade_id": "ex5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "one_hot_cols = ['person_gender', 'employment_type', 'person_emp_exp', 'person_home_ownership', 'loan_intent', 'account_type']\n",
    "ordinal_cols = ['person_education', 'previous_loan_defaults_on_file']\n",
    "categories = [['High School', 'Associate', 'Bachelor', 'Master', 'Doctorate'], ['No', 'Yes']]\n",
    "transformer = ColumnTransformer([('one_hot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), one_hot_cols),\n",
    "                                 ('ordinal', OrdinalEncoder(categories = categories), ordinal_cols)], \n",
    "                                 remainder='passthrough')\n",
    "X_encoded = pd.DataFrame(transformer.fit_transform(X_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.201986Z",
     "start_time": "2024-09-25T16:13:28.197666Z"
    },
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68c62ff4aa3f916e679f67af0307de91",
     "grade": true,
     "grade_id": "test5_1",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.248253Z",
     "start_time": "2024-09-25T16:13:28.246250Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9ae67684e9c7510e7a7547538d2db3c",
     "grade": true,
     "grade_id": "test5_2",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.308861Z",
     "start_time": "2024-09-25T16:13:28.298435Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daa0c93065bc11fd0c17de0f3fd694af",
     "grade": true,
     "grade_id": "test5_3",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7c4bc6e205096143496edcd58600ca2",
     "grade": false,
     "grade_id": "cell-86f80f00088e931d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 6: Standardizing the Features </b> \n",
    "\n",
    "To prevent potential issues with feature scaling, we will standardize the features using a `StandardScaler`. First, instantiate a `StandardScaler` and assign it to the variable `scaler`. Then, test it by transforming the `X_encoded` DataFrame. Store the scaled data in a new DataFrame called `X_scaled`.\n",
    "\n",
    " <br><i>[0.5 points]</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.367475Z",
     "start_time": "2024-09-25T16:13:28.353614Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5d70d0859a7e9513d16031567e1eb90",
     "grade": false,
     "grade_id": "ex6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.403788Z",
     "start_time": "2024-09-25T16:13:28.401270Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f5fca22d2de320e3352ebd2ba6c9da0",
     "grade": true,
     "grade_id": "test6_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.450137Z",
     "start_time": "2024-09-25T16:13:28.446501Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7042906a63097a447edc0a93aa4cbb07",
     "grade": true,
     "grade_id": "test6_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b04c26e267d0b4efea4973ef077e1e5e",
     "grade": false,
     "grade_id": "cell-c7c37a1aa2f33b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 7: Building the Preprocessing Pipeline </b> \n",
    "\n",
    "To complete this part of the assignment, create a `Pipeline` named `pipe` that includes the imputer, transformer, and scaler from the previous exercises. Test the pipeline by transforming the original feature matrix `X`, and store the preprocessed data in a new DataFrame called `X_pipe`.\n",
    "\n",
    "<br><i>[1 points]</i>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-warning'>\n",
    "\n",
    "Be sure you apply the data transformations in the correct order.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.512003Z",
     "start_time": "2024-09-25T16:13:28.492651Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05ebfb30357af4a09ab8968990db619c",
     "grade": false,
     "grade_id": "ex7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('imputer', imputer), \n",
    "                ('transformer', transformer),\n",
    "                ('scaler', scaler)])\n",
    "\n",
    "X_pipe = pd.DataFrame(pipe.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.544483Z",
     "start_time": "2024-09-25T16:13:28.542805Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "759ab0255722dd11069b2d6a92925047",
     "grade": true,
     "grade_id": "test7_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.589900Z",
     "start_time": "2024-09-25T16:13:28.587578Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b4dd682e1b00a58de1d1feb3303bbaa",
     "grade": true,
     "grade_id": "test7_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.635987Z",
     "start_time": "2024-09-25T16:13:28.633835Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b05405708a77a506cbfc2f01ba4903f7",
     "grade": true,
     "grade_id": "test7_3",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:28.683930Z",
     "start_time": "2024-09-25T16:13:28.681474Z"
    },
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "464944bfaff55f7798c11bcc548f3d6d",
     "grade": true,
     "grade_id": "test7_4",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9348626b9858ba769442684ab47e984",
     "grade": false,
     "grade_id": "cell-c7c37a1aa2fjk33bhj36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 8: End-to-End Preprocessing on a Regression problem </b> \n",
    "\n",
    "Now, apply everything you’ve learned to a **regression** problem. Run the next cell to reload the dataset ```df```, making sure it hasn’t been modified during the first part of the assignment.\n",
    "\n",
    "**Your tasks are:**\n",
    "\n",
    "1. **Feature Selection and Engineering:**  \n",
    "   Create a feature matrix `X` and a target array `y` (now using the `credit_score` variable). Drop any irrelevant columns and explain your reasoning for each column you choose to exclude (e.g., loan_status). If you find a column relevant, consider combining it with other existing columns or creating new ones based on the dataset’s features. Be careful when doing this, and remember that you cannot use any information from the test data for feature engineering.\n",
    "\n",
    "2. **Perform train-test split:**\n",
    "   Create two separate datasets: one for training and one for testing, to properly evaluate the performance of your model. Select the evaluation metric that best fits your project or business goals, and provide a clear rationale for the chosen metric.\n",
    "\n",
    "3. **Encoding Categorical and Ordinal Features:**  \n",
    "   Identify the categorical and ordinal columns, and encode them using a `ColumnTransformer` to apply the transformations in parallel.\n",
    "\n",
    "4. **Handling Missing Data:**  \n",
    "   If there are missing values in your feature matrix, decide on an appropriate method to handle them (e.g., mean).\n",
    "\n",
    "5. **Standardizing Features:**  \n",
    "   Assess whether standardization is necessary for your numerical features, and apply it if needed. Justify why you take that desicion.\n",
    "\n",
    "6. **Building a Pipeline:**  \n",
    "   Create a `Pipeline` that integrates all the preprocessing steps you have applied.\n",
    "\n",
    "7. **Select the appropiate regression model:** \n",
    "   Scikit-learn offers several regression models. Try different options to identify a suitable one. There is no need to perform an extensive grid search at this stage, we will cover that in the second assignment.\n",
    "\n",
    "8. **Documentation:**\n",
    "   Remember that thoroughly documenting your code and clearly explaining why certain decisions were made—while also considering and justifying why other options were not chosen—will be highly evaluated. You can use plots to support and justify your decisions.\n",
    "\n",
    "\n",
    "<br><i>[5 points]</i>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:13:29.098631Z",
     "start_time": "2024-09-25T16:13:28.728080Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a995a6fd774d73337f5bf03cab17ba5",
     "grade": false,
     "grade_id": "cell-1c558e3e7a3e6e66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY NOR ADD CODE TO THIS CELL\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jnin/information-systems/refs/heads/main/data/AI1_2025_assignments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0)Doing all the imports that were not done in any previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1)Splitting the features from the target variable   \n",
    "   \n",
    "Person name and bank name are irrelevant as they just describe information that has no predictive value. Credit score is the target variable and therefore obviously disregarded. \n",
    "     \n",
    "Loan status leaks information that could lead the model to learn to decode the score from the status instead of learning the real relationships between the applicant's features and credit score.  \n",
    "  \n",
    "Loan interest rate could leak information as well as that rate might be based on the credit score but since we do not have more information on the relationship between both variables we decided to keep it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = [\"person_name\", \"bank_name\", \"loan_status\", \"credit_score\"])\n",
    "y = df[\"credit_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2)Checking missing values in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(193)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the target variable y has 193 missing values. Therefore we remove all rows for which the target variable is missing so that the model is only trained on complete cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[~y.isna()]\n",
    "y = y.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 193 rows for which the target variable y was missing are now removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Train-Test Split   \n",
    "   \n",
    "We divided the dataset into training and testing sets using an 80/20 split to properly evaluate model performance on unseen data. The training set is used to fit the model, while the test set serves to assess its generalization ability. A fixed random state (29) was set to ensure reproducibility of the results.  \n",
    "  \n",
    "This approach allows a fair and consistent evaluation of model accuracy before deployment, preventing overfitting and ensuring that the performance metrics reflect real predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what the missing values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person_age                        213\n",
       "person_gender                     210\n",
       "person_education                  196\n",
       "employment_type                   226\n",
       "person_income                     196\n",
       "person_emp_exp                    210\n",
       "person_home_ownership             211\n",
       "account_type                      232\n",
       "loan_amnt                         209\n",
       "loan_intent                       190\n",
       "loan_int_rate                     212\n",
       "loan_percent_income               209\n",
       "cb_person_cred_hist_length        201\n",
       "previous_loan_defaults_on_file    208\n",
       "dtype: int64"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = X.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that most features had around 190–230 missing values each, out of roughly 45,000 rows (~0.5%).  \n",
    "This indicates low and likely random missingness, meaning the missing data is not systematically biased.  \n",
    "\n",
    "Because the proportion is small and randomly distributed, simple imputation (mean for numeric, most frequent for categorical) is appropriate.  \n",
    "Dropping rows would unnecessarily remove data and reduce model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if standardization is necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44594.000000</td>\n",
       "      <td>4.461100e+04</td>\n",
       "      <td>44598.000000</td>\n",
       "      <td>44595.000000</td>\n",
       "      <td>44598.000000</td>\n",
       "      <td>44606.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.767570</td>\n",
       "      <td>8.037189e+04</td>\n",
       "      <td>9580.859231</td>\n",
       "      <td>11.006376</td>\n",
       "      <td>0.139672</td>\n",
       "      <td>5.867238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.051365</td>\n",
       "      <td>8.064090e+04</td>\n",
       "      <td>6315.792729</td>\n",
       "      <td>2.980339</td>\n",
       "      <td>0.087199</td>\n",
       "      <td>3.879931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>8.000000e+03</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>5.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>4.720200e+04</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>8.590000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>6.705500e+04</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>11.010000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>9.587750e+04</td>\n",
       "      <td>12241.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>144.000000</td>\n",
       "      <td>7.200766e+06</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         person_age  person_income     loan_amnt  loan_int_rate  \\\n",
       "count  44594.000000   4.461100e+04  44598.000000   44595.000000   \n",
       "mean      27.767570   8.037189e+04   9580.859231      11.006376   \n",
       "std        6.051365   8.064090e+04   6315.792729       2.980339   \n",
       "min       20.000000   8.000000e+03    500.000000       5.420000   \n",
       "25%       24.000000   4.720200e+04   5000.000000       8.590000   \n",
       "50%       26.000000   6.705500e+04   8000.000000      11.010000   \n",
       "75%       30.000000   9.587750e+04  12241.000000      13.000000   \n",
       "max      144.000000   7.200766e+06  35000.000000      20.000000   \n",
       "\n",
       "       loan_percent_income  cb_person_cred_hist_length  \n",
       "count         44598.000000                44606.000000  \n",
       "mean              0.139672                    5.867238  \n",
       "std               0.087199                    3.879931  \n",
       "min               0.000000                    2.000000  \n",
       "25%               0.070000                    3.000000  \n",
       "50%               0.120000                    4.000000  \n",
       "75%               0.190000                    8.000000  \n",
       "max               0.660000                   30.000000  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns = ['person_age', 'person_income', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\n",
    "X[numeric_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive statistics show that the numerical features vary greatly in scale and range, with some variables having much larger values than others. This confirms that standardization is necessary to ensure all features contribute equally to the model and to prevent those with larger ranges from dominating the learning process. We will therefore apply standardization to center and scale all numerical variables before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3,4,5,6)Data Preprocessing Pipelines and Column Transformer  \n",
    "  \n",
    "We encoded categorical and ordinal features using OneHotEncoder and OrdinalEncoder within a ColumnTransformer to apply transformations in parallel. One-hot encoding was used to avoid implying order between categories, while ordinal encoding preserved the logical ranking of ordered variables.  \n",
    "  \n",
    "Missing values were handled using SimpleImputer: the most frequent value for categorical data to maintain consistency, and the mean for numerical data to preserve distribution balance.  \n",
    "  \n",
    "Numerical features were standardized with StandardScaler to ensure all variables are on a comparable scale, preventing features with large ranges from dominating the model.  \n",
    "  \n",
    "Finally, all preprocessing steps were integrated into a single Pipeline to keep the process consistent and reproducible, ensuring identical transformations during training and testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:15:17.022765Z",
     "start_time": "2024-09-25T16:15:16.249897Z"
    },
    "deletable": false,
    "id": "rnxOTovpEqpu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afbce71d8bcd1c8c6a0c2dd3c6e9a686",
     "grade": true,
     "grade_id": "ex8",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ordinal_encoder = OrdinalEncoder(categories = [['High School', 'Associate', 'Bachelor', 'Master', 'Doctorate'], ['No', 'Yes']])\n",
    "scaler = StandardScaler()\n",
    "\n",
    "cat_nominal = ['person_gender', 'employment_type', 'person_emp_exp', 'person_home_ownership', 'account_type', 'loan_intent']\n",
    "cat_ordinal = ['person_education', 'previous_loan_defaults_on_file']\n",
    "num_cols = ['person_age', 'person_income', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\n",
    "\n",
    "cat_nominal_pipe = Pipeline([\n",
    "    ('imputer', imputer_mode),\n",
    "    ('one_hot', one_hot_encoder)])\n",
    "\n",
    "cat_ordinal_pipe = Pipeline([\n",
    "    ('imputer', imputer_mode),\n",
    "    ('ordinal', ordinal_encoder)])\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', imputer_mean), \n",
    "    ('scaling', scaler)])\n",
    "\n",
    "transformer = ColumnTransformer([('cat_nominal', cat_nominal_pipe, cat_nominal),\n",
    "                                 ('cat_binary', cat_ordinal_pipe, cat_ordinal),\n",
    "                                 ('num', num_pipe, num_cols)], \n",
    "                                 remainder = 'passthrough')\n",
    "\n",
    "pipe = Pipeline([('transformer', transformer)])\n",
    "\n",
    "X_train_prepared = pipe.fit_transform(X_train)\n",
    "X_test_prepared = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics: R² and MSE  \n",
    "  \n",
    "We will use R² (Coefficient of Determination) and MSE (Mean Squared Error) to evaluate our regression model.  \n",
    "  \n",
    "R² will show how well the model explains the variability in credit scores, indicating how effectively the features capture the underlying relationships in the data.  \n",
    "  \n",
    "MSE will measure the average squared difference between predicted and actual credit scores, penalizing large errors more strongly.\n",
    "  \n",
    "Together, these metrics will provide a balanced view of model fit and prediction accuracy, ensuring reliable evaluation of our credit score predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Models: Linear, Ridge, and Random Forest  \n",
    "  \n",
    "We will test three regression models: Linear Regression, Ridge Regression, and Random Forest Regressor to identify which performs best for predicting credit scores.  \n",
    "  \n",
    "Linear Regression will be used as a simple baseline to assess linear relationships between the features and the target.  \n",
    "Ridge Regression will help reduce overfitting and address multicollinearity through regularization.  \n",
    "Random Forest will capture more complex and non-linear patterns in the data, improving prediction accuracy.  \n",
    "  \n",
    "By comparing these models, we will evaluate both interpretability and predictive power to select the most appropriate one for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: R² = 0.11804367896678425, MSE = 2304.8954972123433\n",
      "Ridge Regression: R² = 0.1193941044679876, MSE = 2301.36630922104\n",
      "Random Forest: R² = 0.07665564533357205, MSE = 2413.0585548203526\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "rf_reg = RandomForestRegressor(random_state=29)\n",
    "\n",
    "lin_reg.fit(X_train_prepared, y_train)\n",
    "ridge_reg.fit(X_train_prepared, y_train)\n",
    "rf_reg.fit(X_train_prepared, y_train)\n",
    "\n",
    "y_pred_lin = lin_reg.predict(X_test_prepared)\n",
    "y_pred_ridge = ridge_reg.predict(X_test_prepared)\n",
    "y_pred_rf = rf_reg.predict(X_test_prepared)\n",
    "\n",
    "results = {\n",
    "    \"Linear Regression\": (r2_score(y_test, y_pred_lin), mean_squared_error(y_test, y_pred_lin)),\n",
    "    \"Ridge Regression\": (r2_score(y_test, y_pred_ridge), mean_squared_error(y_test, y_pred_ridge)),\n",
    "    \"Random Forest\": (r2_score(y_test, y_pred_rf), mean_squared_error(y_test, y_pred_rf))\n",
    "}\n",
    "\n",
    "for name, (r2, mse) in results.items():\n",
    "    print(f\"{name}: R² = {r2}, MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that all three models have a relatively low R², which means they only explain a small portion of the variability in credit scores. Ridge Regression performs the best and slightly better than Linear Regression, meaning that regularization helps reduce overfitting without significantly changing accuracy. Random Forest performs worse overall which indicates that the dataset may not have any strong non-linear patterns so tree-based models do not really add value. Similar results are found for the MSE values. Overall, the relationships between features and credit scores are weak, and further feature engineering or additional data might be needed to improve model performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session I.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "miba-micromamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
